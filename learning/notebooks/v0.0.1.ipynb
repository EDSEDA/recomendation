{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First version of recommendation model\n",
    "\n",
    "### Links\n",
    "\n",
    "Tutorial: [link](https://www.stepbystepdatascience.com/hybrid-recommender-lightfm-python)\n",
    "\n",
    "Dataset: [instacart-market-basket-analysis](https://www.kaggle.com/datasets/psparks/instacart-market-basket-analysis)\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Download dataset and place files in folder: ../data/instacart-market-basket-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8888/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip setuptools wheel\n",
    "!pip install numpy pandas scikit-learn scipy unidecode optuna plotly nbformat pickle\n",
    "!pip install --no-use-pep517 lightfm # https://github.com/lyst/lightfm/issues/687#issuecomment-1523956355"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "System:\n",
      "    python: 3.11.6 (main, Nov  2 2023, 04:39:40) [Clang 14.0.0 (clang-1400.0.29.202)]\n",
      "executable: /Users/alv.popov/prj/grifon/recommendation/learning/venv/bin/python3.11\n",
      "   machine: macOS-14.1.2-arm64-arm-64bit\n",
      "\n",
      "Python dependencies:\n",
      "      sklearn: 1.4.1.post1\n",
      "          pip: 24.0\n",
      "   setuptools: 69.2.0\n",
      "        numpy: 1.26.4\n",
      "        scipy: 1.12.0\n",
      "       Cython: None\n",
      "       pandas: 2.2.1\n",
      "   matplotlib: 3.8.3\n",
      "       joblib: 1.3.2\n",
      "threadpoolctl: 3.4.0\n",
      "\n",
      "Built with OpenMP: True\n",
      "\n",
      "threadpoolctl info:\n",
      "       user_api: blas\n",
      "   internal_api: openblas\n",
      "    num_threads: 10\n",
      "         prefix: libopenblas\n",
      "       filepath: /Users/alv.popov/prj/grifon/recommendation/learning/venv/lib/python3.11/site-packages/numpy/.dylibs/libopenblas64_.0.dylib\n",
      "        version: 0.3.23.dev\n",
      "threading_layer: pthreads\n",
      "   architecture: armv8\n",
      "\n",
      "       user_api: blas\n",
      "   internal_api: openblas\n",
      "    num_threads: 10\n",
      "         prefix: libopenblas\n",
      "       filepath: /Users/alv.popov/prj/grifon/recommendation/learning/venv/lib/python3.11/site-packages/scipy/.dylibs/libopenblas.0.dylib\n",
      "        version: 0.3.21.dev\n",
      "threading_layer: pthreads\n",
      "   architecture: armv8\n",
      "\n",
      "       user_api: openmp\n",
      "   internal_api: openmp\n",
      "    num_threads: 10\n",
      "         prefix: libomp\n",
      "       filepath: /Users/alv.popov/prj/grifon/recommendation/learning/venv/lib/python3.11/site-packages/sklearn/.dylibs/libomp.dylib\n",
      "        version: None\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "from lightfm.evaluation import precision_at_k, recall_at_k\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from unidecode import unidecode # to deal with accents\n",
    "import pickle\n",
    "\n",
    "import sklearn\n",
    "\n",
    "sklearn.show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>eval_set</th>\n",
       "      <th>order_number</th>\n",
       "      <th>order_dow</th>\n",
       "      <th>order_hour_of_day</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2539329</td>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2398795</td>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>473747</td>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2254736</td>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>431534</td>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3367565</td>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>550135</td>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3108588</td>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2295261</td>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2550362</td>\n",
       "      <td>1</td>\n",
       "      <td>prior</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1187899</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    order_id  user_id eval_set  order_number  order_dow  order_hour_of_day  \\\n",
       "0    2539329        1    prior             1          2                  8   \n",
       "1    2398795        1    prior             2          3                  7   \n",
       "2     473747        1    prior             3          3                 12   \n",
       "3    2254736        1    prior             4          4                  7   \n",
       "4     431534        1    prior             5          4                 15   \n",
       "5    3367565        1    prior             6          2                  7   \n",
       "6     550135        1    prior             7          1                  9   \n",
       "7    3108588        1    prior             8          1                 14   \n",
       "8    2295261        1    prior             9          1                 16   \n",
       "9    2550362        1    prior            10          4                  8   \n",
       "10   1187899        1    train            11          4                  8   \n",
       "\n",
       "    days_since_prior_order  \n",
       "0                      NaN  \n",
       "1                     15.0  \n",
       "2                     21.0  \n",
       "3                     29.0  \n",
       "4                     28.0  \n",
       "5                     19.0  \n",
       "6                     20.0  \n",
       "7                     14.0  \n",
       "8                      0.0  \n",
       "9                     30.0  \n",
       "10                    14.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '../data'\n",
    "dataset_path = path.join(data_path, 'instacart-market-basket-analysis')\n",
    "\n",
    "orders = pd.read_csv(path.join(dataset_path, 'orders.csv'))\n",
    "products = pd.read_csv(path.join(dataset_path, 'products.csv'))\n",
    "aisles = pd.read_csv(path.join(dataset_path, 'aisles.csv'))\n",
    "departments = pd.read_csv(path.join(dataset_path, 'departments.csv'))\n",
    "order_products = pd.concat([pd.read_csv(path.join(dataset_path, 'order_products__prior.csv')),\n",
    "                            pd.read_csv(path.join(dataset_path, 'order_products__train.csv'))])\n",
    "\n",
    "# Check the chronology of the data\n",
    "orders[(orders[\"user_id\"]==1)].sort_values([\"order_number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_name\n",
       "Banana                    472565\n",
       "Bag of Organic Bananas    379450\n",
       "Organic Strawberries      264683\n",
       "Organic Baby Spinach      241921\n",
       "Organic Hass Avocado      213584\n",
       "Organic Avocado           176815\n",
       "Large Lemon               152657\n",
       "Strawberries              142951\n",
       "Limes                     140627\n",
       "Organic Whole Milk        137905\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove accents from product and aisle names\n",
    "products['product_name'] = pd.Series([unidecode(i) for i in products['product_name']])\n",
    "aisles['aisle'] = pd.Series([unidecode(i) for i in aisles['aisle']])\n",
    "\n",
    "# Get the orders + items for each data set\n",
    "def get_orders(subset):\n",
    "    data = orders[[\"user_id\", \"order_id\"]][orders[\"eval_set\"]==subset] \n",
    "    data = data.merge(order_products[[\"order_id\", \"product_id\"]], how='inner', on=\"order_id\")\n",
    "    data = data.merge(products, how='inner', on=\"product_id\")\n",
    "    data = data.merge(aisles, how='inner', on=\"aisle_id\")\n",
    "    data = data.merge(departments, how='inner', on=\"department_id\")\n",
    "    data = data.drop(columns=[\"aisle_id\", \"department_id\"])\n",
    "    return data\n",
    "train_orders = get_orders(\"prior\")\n",
    "test_orders = get_orders(\"train\")\n",
    "\n",
    "# List of all products\n",
    "products = products.merge(aisles, how='inner', on=\"aisle_id\").drop_duplicates()\n",
    "\n",
    "# Which products are bought the most?\n",
    "train_orders[\"product_name\"].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all products purchased by each user\n",
    "def create_train_data(dataset):\n",
    "    data = dataset[[\"user_id\", \"product_name\"]]\n",
    "    \n",
    "    # Add a weight column that scales each interaction by how often the user buys it\n",
    "    data = data.groupby([\"user_id\", \"product_name\"], as_index=False).size()\n",
    "    \n",
    "    data[\"weight\"] = np.where(data[\"size\"]>=5, 5, data[\"size\"]) # cap it at 5\n",
    "    data = data[[\"user_id\", \"product_name\", \"weight\"]]\n",
    "    return data\n",
    "train = create_train_data(train_orders)\n",
    "\n",
    "# Create our test set\n",
    "def create_test_data(test, train):\n",
    "    data = test[[\"user_id\", \"product_name\"]].drop_duplicates()\n",
    "    data = data.merge(train[\"user_id\"].drop_duplicates()) # remove users not in training data\n",
    "    data = data.merge(train[\"product_name\"].drop_duplicates()) # remove items not training data\n",
    "    return data\n",
    "test = create_test_data(test_orders, train) \n",
    "\n",
    "# Create a test set that excludes repurchases\n",
    "def create_new_only_test_data(test, train):                 \n",
    "    data = test.merge(train,  how='left', left_on=['user_id','product_name'], right_on = ['user_id','product_name'])\n",
    "    data = data[data[\"weight\"].isna()]\n",
    "    data = data.drop(columns=[\"weight\"])\n",
    "    return data\n",
    "test_new = create_new_only_test_data(test, train)\n",
    "\n",
    "# unique list of user IDs\n",
    "train_users = train[\"user_id\"].unique()\n",
    "\n",
    "# unique list of prod IDs\n",
    "train_items = train[\"product_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create user, item and feature mappings: (user id map, user feature map, item id map, item feature map)\n",
    "dataset = Dataset() # helper function\n",
    "dataset.fit(train_users, # creates mappings between userIDs and row indices for LightFM\n",
    "                 train_items) \n",
    "len(dataset.mapping()) # we always get 4x mappings out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206209, 49669)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want the user and item mappings (we'll use feature mappings later on)\n",
    "user_mappings = dataset.mapping()[0]\n",
    "item_mappings = dataset.mapping()[2]\n",
    "\n",
    "len(user_mappings), len(item_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0), (2, 1), (3, 2), (4, 3), (5, 4)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the mappings\n",
    "list(user_mappings.items())[:5] # first 5 mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0% Greek Strained Yogurt'),\n",
       " (1, 'Aged White Cheddar Popcorn'),\n",
       " (2, 'Bag of Organic Bananas'),\n",
       " (3, 'Bartlett Pears'),\n",
       " (4, 'Cinnamon Toast Crunch')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create inverse mappings \n",
    "inv_user_mappings = {v:k for k, v in user_mappings.items()}\n",
    "inv_item_mappings = {v:k for k, v in item_mappings.items()}\n",
    "list(inv_item_mappings.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<206209x49669 sparse matrix of type '<class 'numpy.int32'>'\n",
       " \twith 13307839 stored elements in COOrdinate format>,\n",
       " <206209x49669 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 13307839 stored elements in COOrdinate format>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an interactions matrix for each user, item and the weight\n",
    "train_interactions, train_weights = dataset.build_interactions(train[['user_id', 'product_name', 'weight']].values)\n",
    "train_interactions, train_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[1, 1, 1, ..., 0, 0, 0],\n",
       "         [0, 0, 1, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 1, ..., 0, 0, 0],\n",
       "         [0, 0, 1, ..., 0, 0, 0],\n",
       "         [0, 0, 1, ..., 0, 0, 0]], dtype=int32),\n",
       " matrix([[1., 2., 2., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 5., ..., 0., 0., 0.],\n",
       "         [0., 0., 5., ..., 0., 0., 0.],\n",
       "         [0., 0., 5., ..., 0., 0., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the matrices\n",
    "train_interactions.todense(), train_weights.todense() # weights and interactions are the same if we just use 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<206209x49669 sparse matrix of type '<class 'numpy.int32'>'\n",
       " \twith 555776 stored elements in COOrdinate format>,\n",
       " <206209x49669 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 555776 stored elements in COOrdinate format>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Test set - notice that LightFM automatically makes it the same size as Train to preserve integer mappings\n",
    "test_interactions, test_weights = dataset.build_interactions(test[['user_id', 'product_name']].values)\n",
    "test_interactions, test_weights\n",
    "\n",
    "# Create a new-products-purchased-only Test set\n",
    "test_new_interactions, test_new_weights = dataset.build_interactions(test_new[['user_id', 'product_name']].values)\n",
    "test_new_interactions, test_new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [03:56<00:00, 11.81s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x16db33490>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LightFM(no_components=10,  # the dimensionality of the feature latent embeddings\n",
    "                \t\t\tlearning_schedule='adagrad', # type of optimiser to use\n",
    "                \t\t\tloss='warp', # loss type\n",
    "                \t\t\tlearning_rate=0.05, # set the initial learning rate\n",
    "                \t\t\titem_alpha=0.0, # L2 penalty on item features\n",
    "                \t\t\tuser_alpha=0.0, # L2 penalty on users features \n",
    "                \t\t\tmax_sampled=10, # maximum number of negative samples used during WARP fitting\n",
    "                \t\t\trandom_state=123)\n",
    "             \n",
    "model.fit(train_interactions, # our training data\n",
    "               epochs = 2,\n",
    "               verbose=True)\n",
    "\n",
    "# Measure how well it did in the Test period\n",
    "for metric in [precision_at_k, recall_at_k]:\n",
    "    # Get the precision and recall for Train and Test\n",
    "    for data, name in [(train_interactions, \"Train\"), (test_interactions, \"Test\")]:\n",
    "        print(f\"{name} {metric.__name__}: %.2f\" % \n",
    "              metric(model, data, k=10).mean())\n",
    "\n",
    "    # What about for just new-to-user purchases?\n",
    "    print(f\"Test new {metric.__name__}: %.2f\" % \n",
    "          metric(\n",
    "              model,\n",
    "              test_new_interactions, \n",
    "              train_interactions=train_interactions, # supress previously bought prods from being recommended\n",
    "              k=10\n",
    "              ).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.466873</td>\n",
       "      <td>1.109881</td>\n",
       "      <td>-2.672024</td>\n",
       "      <td>1.289624</td>\n",
       "      <td>-2.252262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.466969</td>\n",
       "      <td>-0.612912</td>\n",
       "      <td>1.708214</td>\n",
       "      <td>2.160152</td>\n",
       "      <td>-0.745130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.199134</td>\n",
       "      <td>-0.412123</td>\n",
       "      <td>-1.790381</td>\n",
       "      <td>2.719636</td>\n",
       "      <td>-1.153012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.354126</td>\n",
       "      <td>-1.335109</td>\n",
       "      <td>0.527995</td>\n",
       "      <td>2.578909</td>\n",
       "      <td>-0.627769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.447633</td>\n",
       "      <td>0.308471</td>\n",
       "      <td>-0.503776</td>\n",
       "      <td>0.762689</td>\n",
       "      <td>-1.477878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4\n",
       "0 -0.466873  1.109881 -2.672024  1.289624 -2.252262\n",
       "1 -2.466969 -0.612912  1.708214  2.160152 -0.745130\n",
       "2 -1.199134 -0.412123 -1.790381  2.719636 -1.153012\n",
       "3 -1.354126 -1.335109  0.527995  2.578909 -0.627769\n",
       "4 -1.447633  0.308471 -0.503776  0.762689 -1.477878"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create all user and item matrix to get predictions for it\n",
    "n_users, n_items = train_interactions.shape\n",
    "\n",
    "# Force lightFM to create predictions for all users and all items\n",
    "scoring_user_ids = np.concatenate([np.full((n_items, ), i) for i in range(n_users)]) # repeat user ID for number of prods\n",
    "scoring_item_ids = np.concatenate([np.arange(n_items) for i in range(n_users)]) # repeat entire range of item IDs x number of user\n",
    "scores = model.predict(user_ids = scoring_user_ids, \n",
    "                                     item_ids = scoring_item_ids)\n",
    "scores = scores.reshape(-1, n_items) # get 1 row per user\n",
    "recommendations = pd.DataFrame(scores)\n",
    "recommendations.shape\n",
    "\n",
    "# Have a look at the predicted scores for the first 5 users and first 5 items\n",
    "recommendations.iloc[:5,:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load latent representations to try computing predictions manually\n",
    "item_biases, item_embeddings = model.get_item_representations()\n",
    "user_biases, user_embeddings = model.get_user_representations()\n",
    "\n",
    "#Combine item_embeddings with biases for dot product\n",
    "manual_scores = ((user_embeddings @ item_embeddings.T + item_biases).T + user_biases).T\n",
    "manual_scores.shape\n",
    "\n",
    "# They match apart from some tiny rounding!\n",
    "np.allclose(manual_scores, scores, rtol=0, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous purchases:\n",
      "Cabernet Sauvignon\n",
      "Petite Sirah\n",
      "Merlot\n",
      "Malbec\n",
      "Essential Red\n",
      "Sauvignon Blanc, California, 2011\n",
      "Natural White Organic Wine\n",
      "Pinot Grigio, California, 2011\n",
      "Top 10 recommendations:\n",
      "('Petite Sirah', 0)\n",
      "('Malbec', 1)\n",
      "('Pinot Noir', 2)\n",
      "('Chardonnay', 3)\n",
      "('Sauvignon Blanc', 4)\n",
      "('Pinot Noir California', 5)\n",
      "('Merlot', 6)\n",
      "('Pinot Grigio', 7)\n",
      "('Old Vine Zinfandel', 8)\n",
      "('Sauvignon Blanc, California, 2011', 9)\n"
     ]
    }
   ],
   "source": [
    "# Top 10 predictions for every user\n",
    "k=10\n",
    "top_10 = np.argsort(-scores, axis=1) [::, :k] \n",
    "\n",
    "# Get the previous purchases for every user\n",
    "previous = np.array(train_interactions.todense())\n",
    "\n",
    "# Get the previous purchases and the top predictions for user 206114\n",
    "user = user_mappings.get(206114) \n",
    "\n",
    "print(\"Previous purchases:\", *[inv_item_mappings.get(key) for key in np.array(range(previous.shape[1]))[previous[user]>0]], sep=\"\\n\")\n",
    "print(\"Top 10 recommendations:\", *sorted(zip([inv_item_mappings.get(key) for key in top_10[user]], range(k)), key = lambda x: x[1]), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 less popular recommendations:\n",
      "('Petite Sirah', 0)\n",
      "('Organic Zero Sulfites Red Wine', 1)\n",
      "('Organic Mendocino Cabernet Sauvignon', 2)\n",
      "('Chenin Blanc', 3)\n",
      "('Pinot Noir, California 2010', 4)\n",
      "('Pinot Grigio, California, 2011', 5)\n",
      "('Sauvignon Blanc, California, 2011', 6)\n",
      "('Pinot Noir California', 7)\n",
      "('Natural White Organic Wine', 8)\n",
      "('Gruner Veltliner', 9)\n"
     ]
    }
   ],
   "source": [
    "without_biases = (model.user_embeddings @ model.item_embeddings.T)\n",
    "without_biases\n",
    "\n",
    "top_10_without_biases = np.argsort(-without_biases, axis=1) [::, :k] \n",
    "print(\"Top 10 less popular recommendations:\", *sorted(zip([inv_item_mappings.get(key) for key in top_10_without_biases[user]], range(k)), key = lambda x: x[1]), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train precision_at_k: 0.14\n",
      "Test  precision_at_k: 0.05\n",
      "Test new precision_at_k: 0.011\n",
      "Train recall_at_k: 0.58\n",
      "Test  recall_at_k: 0.38\n",
      "Test new recall_at_k: 0.072\n"
     ]
    }
   ],
   "source": [
    "# Can also set model biases to be 0 with\n",
    "model.item_biases *= 0.0 # and then can use predict() as normal\n",
    "\n",
    "# Measure how well it did in the Test period\n",
    "for metric in [precision_at_k, recall_at_k]:\n",
    "    # Get the precision and recall for Train and Test\n",
    "    for data, name in [(train_interactions, \"Train\"), (test_interactions, \"Test \")]:\n",
    "        print(f\"{name} {metric.__name__}: %.2f\" % \n",
    "              metric(model,\n",
    "                     data, \n",
    "                     k=10).mean())\n",
    "        \n",
    "    # What about for just new-to-user purchases?\n",
    "    print(f\"Test new {metric.__name__}: %.3f\" % \n",
    "          metric(model,\n",
    "                 test_new_interactions, \n",
    "                 train_interactions=train_interactions, # supress previously bought prods from being recommended\n",
    "                 k=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar items\n",
    "\n",
    "def get_similar(model):\n",
    "    # Extract the user and item representations\n",
    "    _, item_embeddings  = model.get_item_representations()\n",
    "\n",
    "    item_to_item = pd.DataFrame(cosine_similarity(item_embeddings))\n",
    "    item_to_item.index = item_mappings.keys()\n",
    "    item_to_item.columns = item_mappings.keys()\n",
    "    item_to_item\n",
    "\n",
    "get_similar(model)['Banana'].sort_values(ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# Define our hyperparameter seearch space\n",
    "def objective(trial):\n",
    "    \n",
    "    # Use LightFMs inbuilt train-test split function to create train and validation subsets\n",
    "    train, val = random_train_test_split(train_interactions, test_percentage=0.25, random_state=42)\n",
    "    \n",
    "    # Define the hyperparameter space\n",
    "    param = {\n",
    "        'no_components': trial.suggest_int(\"no_components\", 5, 64),\n",
    "        \"learning_schedule\": trial.suggest_categorical(\"learning_schedule\", [\"adagrad\", \"adadelta\"]),\n",
    "        \"loss\":  trial.suggest_categorical(\"loss\", [\"bpr\", \"warp\", \"warp-kos\"]),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 1),\n",
    "        \"item_alpha\": trial.suggest_float(\"item_alpha\", 1e-10, 1e-06, log=True),\n",
    "        \"user_alpha\": trial.suggest_float(\"user_alpha\", 1e-10, 1e-06, log=True), \n",
    "        \"max_sampled\": trial.suggest_int(\"max_sampled\", 5, 15),\n",
    "    }\n",
    "    epochs = trial.suggest_int(\"epochs\", 20, 50)\n",
    "    \n",
    "    model = LightFM(**param, random_state=123) \n",
    "    model.fit(train, \n",
    "              epochs = epochs,\n",
    "              verbose=True)\n",
    "    \n",
    "    val_precision = precision_at_k(model, \n",
    "                                   val, \n",
    "                                   train_interactions=train,\n",
    "                                   k=10).mean()\n",
    "\n",
    "    return val_precision\n",
    "\n",
    "# Define the study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "# Add in our original hyperparmeter values as a starting point for Optuna\n",
    "study.enqueue_trial(params={\"no_components\":10, \n",
    "                            \t\t\t\t\t\"learning_schedule\":'adagrad', \n",
    "                            \t\t\t\t\t\"loss\":'warp',\n",
    "                            \t\t\t\t\t\"learning_rate\":0.05,\n",
    "                            \t\t\t\t\t\"item_alpha\":1e-10, \n",
    "                            \t\t\t\t\t\"user_alpha\":1e-10, \n",
    "                            \t\t\t\t\t\"max_sampled\":10,\n",
    "                            \t\t\t\t\t\"epochs\":20})\n",
    "\n",
    "# Run the optimisation        \n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Save the best parameters\n",
    "best_params = study.best_params\n",
    "for k, v in best_params.items():\n",
    "    print(k,\":\",v)\n",
    "\n",
    "# Which parameters were the most important?\n",
    "optuna.importance.get_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy up epochs as not a parameter to be passed to LightFM() directly\n",
    "num_epochs = best_params['epochs'] # save best epochs as a separate object\n",
    "del best_params['epochs'] # then remove it from best_params object\n",
    "\n",
    "# Train with the best parameters\n",
    "model = LightFM(**best_params, random_state=123)\n",
    "\n",
    "model.fit(train_interactions, \n",
    "          epochs = num_epochs,\n",
    "          verbose=True)\n",
    "\n",
    "# Measure how well it did in the Test period\n",
    "for metric in [precision_at_k, recall_at_k]:\n",
    "    # Get the precision and recall for Train and Test\n",
    "    for data, name in [(train_interactions, \"Train\"), (test_interactions, \"Test \")]:\n",
    "        print(f\"{name} {metric.__name__}: %.2f\" % \n",
    "              metric(model,\n",
    "                     data, \n",
    "                     k=10).mean())\n",
    "        \n",
    "    # What about for just new-to-user purchases?\n",
    "    print(f\"Test new {metric.__name__}: %.3f\" % \n",
    "          metric(model,\n",
    "                 test_new_interactions, \n",
    "                 train_interactions=train_interactions, # supress previously bought prods from being recommended\n",
    "                 k=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar(model)['Banana'].sort_values(ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    train, val = random_train_test_split(train_interactions, test_percentage=0.25, random_state=42)\n",
    "    \n",
    "    param = {\n",
    "        'no_components': trial.suggest_int(\"no_components\", 5, 64),\n",
    "        \"learning_schedule\": trial.suggest_categorical(\"learning_schedule\", [\"adagrad\", \"adadelta\"]),\n",
    "        \"loss\":  trial.suggest_categorical(\"loss\", [\"warp\"]),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 1),\n",
    "        \"item_alpha\": trial.suggest_float(\"item_alpha\", 1e-10, 1e-06, log=True),\n",
    "        \"user_alpha\": trial.suggest_float(\"user_alpha\", 1e-10, 1e-06, log=True),\n",
    "        \"max_sampled\": trial.suggest_int(\"max_sampled\", 5, 15),\n",
    "    }\n",
    "    epochs = trial.suggest_int(\"epochs\", 20, 50)\n",
    "    sample_weights = trial.suggest_categorical(\"sample_weight\", [\"None\", \"train_weights\"]) # add weights as a parameter  \n",
    "    \n",
    "    model = LightFM(**param, random_state=123) \n",
    "    model.fit(train, \n",
    "              sample_weight=eval(sample_weights),\n",
    "              epochs = epochs,\n",
    "              verbose=True)\n",
    "    \n",
    "    val_precision = precision_at_k(model, \n",
    "                                   val, \n",
    "                                   train_interactions=train,\n",
    "                                   k=10).mean()\n",
    "\n",
    "    return val_precision\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "# Add in our original hyperparmeter values as a starting point for Optuna\n",
    "best_params[\"epochs\"]=num_epochs # manually add epochs\n",
    "best_params[\"sample_weight\"] =\"None\" # add in the fact the previous models didn't use weights\n",
    "best_params[\"loss\"] =\"warp\" #can't use kos with weights so switch it to warp\n",
    "study.enqueue_trial(best_params)\n",
    "\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "best_params = study.best_params\n",
    "for k, v in best_params.items():\n",
    "    print(k,\":\",v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = best_params['epochs']\n",
    "sample_weights=best_params['sample_weight']\n",
    "    \n",
    "del best_params['epochs']\n",
    "del best_params['sample_weight']\n",
    "\n",
    "# Train with the best parameters\n",
    "model = LightFM(**best_params, random_state=123)\n",
    "\n",
    "model.fit(train_interactions, \n",
    "          sample_weight=eval(sample_weights),\n",
    "          epochs = num_epochs,\n",
    "          verbose=True)\n",
    "\n",
    "# Measure how well it did in the Test period\n",
    "for metric in [precision_at_k, recall_at_k]:\n",
    "    # Get the precision and recall for Train and Test\n",
    "    for data, name in [(train_interactions, \"Train\"), (test_interactions, \"Test \")]:\n",
    "        print(f\"{name} {metric.__name__}: %.2f\" % \n",
    "              metric(model,\n",
    "                     data, \n",
    "                     k=10).mean())\n",
    "        \n",
    "    # What about for just new-to-user purchases?\n",
    "    print(f\"Test new {metric.__name__}: %.3f\" % \n",
    "          metric(model,\n",
    "                 test_new_interactions, \n",
    "                 train_interactions=train_interactions, # supress previously bought prods from being recommended\n",
    "                 k=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../artifacts/v0.0.1', 'w') as file:\n",
    "    pickle.dump(model, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
